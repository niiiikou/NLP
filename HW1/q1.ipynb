{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from collections import Counter\n",
        "from tabulate import tabulate\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "u2LRPgDa5blq"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T97WZ-AXwBu1",
        "outputId": "2ff68c61-6348-4dcb-ad2f-9e403be42597"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ],
      "source": [
        "nltk.download('punkt')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with open (\"hp_fa.txt\", \"r\") as f:\n",
        "    text = f.read()"
      ],
      "metadata": {
        "id": "L-Gkyg0_8aK-"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "! git clone https://github.com/kharazi/persian-stopwords.git\n",
        "%cd persian-stopwords/"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1P8uudpJff5D",
        "outputId": "fb027abf-a30c-4f91-c07f-7a7cca9ba27c"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'persian-stopwords'...\n",
            "remote: Enumerating objects: 51, done.\u001b[K\n",
            "remote: Counting objects: 100% (9/9), done.\u001b[K\n",
            "remote: Compressing objects: 100% (4/4), done.\u001b[K\n",
            "remote: Total 51 (delta 7), reused 5 (delta 5), pack-reused 42\u001b[K\n",
            "Unpacking objects: 100% (51/51), 34.96 KiB | 483.00 KiB/s, done.\n",
            "/content/persian-stopwords\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with open (\"persian\", \"r\") as f:\n",
        "    fa_StopWords = f.read()\n",
        "fa_StopWords = fa_StopWords.replace('\\ufeff', '')\n",
        "fa_StopWords = fa_StopWords.split(\"\\n\")\n",
        "fa_punct = fa_StopWords[:18]"
      ],
      "metadata": {
        "id": "dLU3jtNlfjCE"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sentences = nltk.sent_tokenize(text)"
      ],
      "metadata": {
        "id": "hPzz0U98pAB4"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sentences"
      ],
      "metadata": {
        "id": "6fofFnGtgjNI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_ngram(input_txt, n):\n",
        "\n",
        "  def remove_punct(sentence):\n",
        "    for i in sentence:\n",
        "      #print(\"i :\",i)\n",
        "      #print(\"ss: \",ss)\n",
        "      if i in fa_punct:\n",
        "        sentence = sentence.replace(i, '')\n",
        "    return sentence\n",
        "\n",
        "  #split sentences\n",
        "  sentences = nltk.sent_tokenize(input_txt)\n",
        "\n",
        "  all_sentences_tok = []\n",
        "\n",
        "  for sent in sentences:\n",
        "    # preproces\n",
        "    sent = sent.replace('\\ufeff', '')\n",
        "    sent = sent.replace('\\xa0', ' ')\n",
        "    sent = remove_punct(sent)\n",
        "\n",
        "    # \n",
        "    new_sent = nltk.word_tokenize(sent)\n",
        "    \n",
        "    all_sentences_tok.append(new_sent)\n",
        "    #print(\"new_sent :\", all_sentences_tok)\n",
        "  \n",
        "  sentences_padn = [list(nltk.lm.preprocessing.pad_both_ends(sent,  2)) \n",
        "                        for sent in all_sentences_tok]\n",
        "\n",
        "  if n > 2 :\n",
        "    sentences_padn_new = []\n",
        "    for sen in sentences_padn:\n",
        "      s = ['<s>'] * (n-1)\n",
        "      sentences_padn_new.append(s + sen)\n",
        "    sentences_padn = sentences_padn_new\n",
        "\n",
        "  \n",
        "  sentences_ngram = [list(nltk.ngrams(sent, n)) \n",
        "                        for sent in sentences_padn]\n",
        "  \n",
        "\n",
        "  sentences_ngram_minus = [list(nltk.ngrams(sent, n-1)) \n",
        "                        for sent in sentences_padn]\n",
        "\n",
        "  flat_ngram = sum(sentences_ngram, [])\n",
        "  flat_ngram_minus = sum(sentences_ngram_minus, [])  \n",
        "\n",
        "  return sentences_ngram, sentences_ngram_minus, flat_ngram, flat_ngram_minus           \n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def get_conditional_prob(ngrams_list, ngram_count, n_1_gram_count):\n",
        " \n",
        "  probs = {}\n",
        "  for bi in ngrams_list:\n",
        "\n",
        "    cond = round((ngram_count[bi] + 1 ) / ( n_1_gram_count[bi[0]] + len(n_1_gram_count)), 6)\n",
        "    \n",
        "    probs[bi] = cond\n",
        "    \n",
        "  return(probs)\n",
        "\n",
        "\n",
        "\n",
        "def find_max_prob(l, ngram_probs):\n",
        "\n",
        "  b_tokens = {}\n",
        "  max_l = []\n",
        "  \n",
        "  for k in ngram_probs :\n",
        "    \n",
        "    if k[0:len(l)] == tuple(l):\n",
        "      \n",
        "      b_tokens[k[len(l):][0]] = ngram_probs[k]\n",
        "\n",
        "  sorted_list = sorted(b_tokens.items(), key=lambda item: item[1], reverse=True)\n",
        "  \n",
        "  for i in sorted_list[:4]:\n",
        "    max_l.append(i[0])\n",
        "\n",
        "\n",
        "  return max_l\n",
        "\n"
      ],
      "metadata": {
        "id": "n8RNPb7-dt1M"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_sentence(text, n):\n",
        "\n",
        "  sentences_ngram, sentences_n_1gram, flat_ngram, flat_n_1gram = generate_ngram(text, n)\n",
        "  ngram_count = Counter(flat_ngram)\n",
        "  n_1gram_count = Counter(flat_n_1gram)\n",
        "\n",
        "  ngram_probs = get_conditional_prob(flat_ngram, ngram_count, n_1gram_count)\n",
        "  \n",
        "  gen_sentence = str()\n",
        "  saw_letters = []\n",
        "  nex_l_count = 0\n",
        "\n",
        "  for i in range(24):\n",
        "\n",
        "    if i < n-1 :\n",
        "      letter = '<s>'\n",
        "      gen_sentence += ' '\n",
        "      gen_sentence += letter\n",
        "      saw_letters.append(letter)\n",
        "      nex_l_count +=1\n",
        "\n",
        "    else:\n",
        "    \n",
        "      next_l = np.random.choice(find_max_prob(saw_letters[-(n-1):], ngram_probs))\n",
        "      \n",
        "      gen_sentence += ' '\n",
        "      gen_sentence += next_l\n",
        "      nex_l_count +=1\n",
        "      saw_letters.append(next_l)\n",
        "      letter= next_l\n",
        "\n",
        "      if letter == '</s>' :\n",
        "        break\n",
        "\n",
        "\n",
        "  return gen_sentence, nex_l_count \n"
      ],
      "metadata": {
        "id": "oEbags5ybUTQ"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_sen = 5\n",
        "num = 0\n",
        "n =2\n",
        "while True:\n",
        "  sen_gen, count_sen_gen = generate_sentence(text, n)\n",
        "  if 12 <= count_sen_gen <=24 :\n",
        "    print(sen_gen)\n",
        "    num+=1\n",
        "  if num == 5 :\n",
        "    break"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NQvkaSmvbXab",
        "outputId": "d8211716-e29b-4387-d5a6-5e989c3e3618"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " <s> در همان وقت ها را روي زمين افتاد و از اين جا دور خود را از آن را به هري به او به\n",
            " <s> هاگريد با هم به نظر ميرسيد که در همان وقت به هري به او به او را از روي زمين کشيده شدن هري\n",
            " <s> در آن گاه در آن جا دور خود نشان داد و با حالتي عصبي و گفت من خودم شنيدم که از روي آن\n",
            " <s> رون به نظر ميرسيد هيچ وقت ها رو نگاه کرد که از اين جا </s>\n",
            " <s> هاگريد با صداي بلند گفت که در اين بار ديگر با اين کار را روي زمين کشيده و به او به هري گفت\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "num_sen = 5\n",
        "num = 0\n",
        "n = 3\n",
        "while True:\n",
        "  sen_gen, count_sen_gen = generate_sentence(text, n)\n",
        "  if 12 <= count_sen_gen <=24 :\n",
        "    print(sen_gen)\n",
        "    num+=1\n",
        "  if num == 5 :\n",
        "    break\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yCORDnRQbhfO",
        "outputId": "ea94ab40-0500-468c-ef8d-a5c1ee71ec25"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " <s> <s> <s> رون به او نزديک تر شدند هاگريد فرياد زد گريفندور </s>\n",
            " <s> <s> هري گفت خب که چي مي خواي نگذاري پسر ليلي و جيمز نه باورم نميشه که بالاخره طرف رفته باشه قرمز مي\n",
            " <s> <s> <s> <s> رون با عصبانيت از هري پاتر هري ميخواست با عجله به طرف در رفت وآمد بودند و به نظر نميرسيد\n",
            " <s> <s> هري به رون گفت اگه خودش دوست داشته باشه بذار ببينم دامبلدور فلوفي رو بزرگ کرده م بزرگ کردن اژدها ديگه برم\n",
            " <s> <s> در همان حال رداهاي جادوگريشان را درآوردند و کت يا ژاکت پوشيدند </s>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "num_sen = 5\n",
        "num = 0\n",
        "n = 5\n",
        "while True:\n",
        "  sen_gen, count_sen_gen = generate_sentence(text, n)\n",
        "  if 12 <= count_sen_gen <=24 :\n",
        "    print(sen_gen)\n",
        "    num+=1\n",
        "  if num == 5 :\n",
        "    break\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UiZ8q2lWbr8O",
        "outputId": "faa0c61f-b058-4da9-ac34-d25d5f561722"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " <s> <s> <s> <s> <s> <s> <s> هري به سرعت اوج ميگرفت و به سمت حلقه ي دروازه شيرجه ميرفت آن گاه از سوي\n",
            " <s> <s> <s> <s> در همان لحظه يک نفر از جلوي در قطار شروع به صحبت کرد اما سيوس فينيگان موحنايي به ميان حرف\n",
            " <s> <s> <s> <s> <s> رون گفت به نظر شما اگه بخوايم از اين جا رد بشيم بهمون حمله ميکنن هري گفت ممکنه ظاهراً\n",
            " <s> <s> <s> <s> رون به هري گفت همين طور بزن </s>\n",
            " <s> <s> <s> <s> <s> هري با اين که مطمئن نبود مار بتواند حرف او را بفهمد از لاي شيشه آهسته گفت مي دونم\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}